{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from typing import TypedDict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize APIs\n",
    "youtube = build('youtube', 'v3', developerKey=os.getenv(\"YOUTUBE_API_KEY\"))\n",
    "embedder = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchResult:\n",
    "    def __init__(self, search_result):\n",
    "        self.video_id = search_result['id']['videoId']\n",
    "        self.title = search_result['snippet']['title']\n",
    "        self.transcript = self._get_transcript()\n",
    "\n",
    "    def _get_transcript(self):\n",
    "        try:\n",
    "            transcript_list = YouTubeTranscriptApi.get_transcript(self.video_id)\n",
    "            return \" \".join([item['text'] for item in transcript_list])\n",
    "        except Exception as e:\n",
    "            print(f\"Transcript error for {self.video_id}: {str(e)}\")\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_yt(query, max_results=3):\n",
    "    try:\n",
    "        request = youtube.search().list(\n",
    "            part=\"snippet\",\n",
    "            maxResults=max_results,\n",
    "            q=query,\n",
    "            videoCaption='closedCaption',\n",
    "            type='video',\n",
    "        )\n",
    "        return request.execute().get('items', [])\n",
    "    except Exception as e:\n",
    "        print(f\"YouTube API error: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(items):\n",
    "    documents = []\n",
    "    for item in items:\n",
    "        result = SearchResult(item)\n",
    "        if not result.transcript:\n",
    "            continue\n",
    "            \n",
    "        full_text = f\"Title: {result.title}\\nTranscript: {result.transcript}\"\n",
    "        chunks = text_splitter.split_text(full_text)\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            documents.append(Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"video_id\": result.video_id,\n",
    "                    \"title\": result.title\n",
    "                }\n",
    "            ))\n",
    "    \n",
    "    if not documents:\n",
    "        print(\"No valid documents created\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        return FAISS.from_documents(documents, embedder)\n",
    "    except Exception as e:\n",
    "        print(f\"FAISS error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    author: str\n",
    "    question: str\n",
    "    combined_index: Optional[FAISS]\n",
    "    objective_check: bool\n",
    "    response_summary:bool\n",
    "    validate_response: str\n",
    "    fact_correction:str\n",
    "    generate_final_response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_details(state=None) -> State:\n",
    "    return {\n",
    "        \"topic\": \"Rich Dad Poor Dad\",\n",
    "        \"author\": \"Robert Kiyosaki\",\n",
    "        \"question\": \"\",\n",
    "        \"combined_index\": None,\n",
    "        \"objective_check\": \"\",\n",
    "        \"response_summary\":\"\",\n",
    "        \"validate_response\": \"\",\n",
    "        \"fact_correction\":\"\",\n",
    "        \"generate_final_response\": \"\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_question(state: State) -> State:\n",
    "    return {\n",
    "        **state,\n",
    "        \"question\": \"current economic condition what is the best investment?\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_youtube_items(items):\n",
    "    documents = []\n",
    "    for item in items:\n",
    "        result = SearchResult(item)\n",
    "        if result.transcript:\n",
    "            full_text = f\"Video: {result.title}\\nTranscript: {result.transcript}\"\n",
    "            chunks = text_splitter.split_text(full_text)\n",
    "            documents.extend([\n",
    "                Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\n",
    "                        \"source\": \"youtube\",\n",
    "                        \"video_id\": result.video_id,\n",
    "                        \"title\": result.title\n",
    "                    }\n",
    "                ) for chunk in chunks\n",
    "            ])\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ddg_results(results):\n",
    "    return [\n",
    "        Document(\n",
    "            page_content=result,\n",
    "            metadata={\n",
    "                \"source\": \"web\",\n",
    "                \"search_query\": state[\"topic\"] + \" \" + state[\"author\"]\n",
    "            }\n",
    "        ) for result in results.split(\"\\n\\n\") if result.strip()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_combined_index(state: State) -> State:\n",
    "    try:\n",
    "        print(\"\\n=== Building Combined Knowledge Base ===\")\n",
    "        \n",
    "        # 1. Get YouTube data\n",
    "        yt_items = search_yt(f\"{state['topic']} {state['author']}\")\n",
    "        yt_docs = process_youtube_items(yt_items)\n",
    "        \n",
    "        # 2. Get DuckDuckGo data\n",
    "        ddg_results = DuckDuckGoSearchRun().invoke(\n",
    "            f\"{state['topic']} {state['author']} recent articles/interviews\"\n",
    "        )\n",
    "        ddg_docs = process_ddg_results(ddg_results)\n",
    "        \n",
    "        # 3. Combine and index\n",
    "        all_docs = yt_docs + ddg_docs\n",
    "        if not all_docs:\n",
    "            raise ValueError(\"No documents found from any source\")\n",
    "            \n",
    "        state[\"combined_index\"] = FAISS.from_documents(all_docs, embedder)\n",
    "        print(f\"✅ Created combined index with {len(all_docs)} documents\")\n",
    "        return state\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Index build error: {str(e)}\")\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_author_response(state: State) -> State:\n",
    "    try:\n",
    "        print(\"\\n=== Generating Author-Style Response ===\")\n",
    "        \n",
    "        template = \"\"\"As {author}'s analytical clone specializing in {topic}, my perspective is:\n",
    "\n",
    "        Context: {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Requirements:\n",
    "        1. Maintain {author}'s signature style\n",
    "        2. Integrate both historical and current insights\n",
    "        3. Highlight practical applications\n",
    "        4. Acknowledge limitations where applicable\n",
    "        \n",
    "        Analysis:\"\"\"\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        rag_chain = (\n",
    "            {\n",
    "            \"context\": state[\"combined_index\"].as_retriever(search_kwargs={\"k\": 5}),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "            \"author\": lambda _: state[\"author\"],\n",
    "            \"topic\": lambda _: state[\"topic\"]        \n",
    "            }\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        \n",
    "        state[\"response_summary\"] = rag_chain.invoke(state[\"question\"])\n",
    "        return state\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Generation error: {str(e)}\")\n",
    "        state[\"response_summary\"] = \"Error generating response\"\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_user_input(state: State):\n",
    "    validation_prompt = f\"\"\"**Relevance Validation Task**\n",
    "    \n",
    "    Author: {state['author']}\n",
    "    Topic: {state['topic']}\n",
    "    Question: {state['question']}\n",
    "\n",
    "    **Rules:**\n",
    "    1. Focus on financial/economic concepts for \"Rich Dad Poor Dad\" topics\n",
    "    2. Consider {state['author']}'s known expertise\n",
    "    3. Allow tangential but related concepts (e.g., assets, investments, cash flow)\n",
    "    4. Reject completely unrelated topics (e.g., romance, sports, politics)\n",
    "\n",
    "    **Examples:**\n",
    "    - Topic: Assets | Question: \"Is a house an asset?\" → True\n",
    "    - Topic: Investing | Question: \"Best stocks in 2024?\" → True  \n",
    "    - Topic: Cash Flow | Question: \"How to find true love?\" → False\n",
    "\n",
    "    **Decision:**\n",
    "    Respond ONLY with 'True' or 'False'\"\"\"\n",
    "\n",
    "    print(f\"Validating: '{state['question']}' against topic: '{state['topic']}'\")\n",
    "\n",
    "    response = llm.invoke(validation_prompt).content.strip().lower()\n",
    "    is_valid = response == \"true\"\n",
    "        \n",
    "    print(f\"Validation result: {is_valid} | Raw response: '{response}'\")\n",
    "    return {\"objective_check\": is_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def validate_facts(state: State):\n",
    "    print(f\"\\n=== Validating YouTube Summary for '{state['question']}' ===\")\n",
    "        \n",
    "    feedback_prompt = f\"\"\"**Validation Task: Summary Quality Check**\n",
    "\n",
    "    [Author] {state['author']}\n",
    "    [Topic] {state['topic']}\n",
    "    [User Question] {state['question']}\n",
    "\n",
    "    [Quality Criteria]\n",
    "    1. Directly answers the specific question asked\n",
    "    2. Uses {state['author']}'s signature communication style\n",
    "    3. Contains concrete examples/data from source material\n",
    "    4. Acknowledges limitations when information is missing\n",
    "\n",
    "    [Summary to Validate]\n",
    "    {state['response_summary']}\n",
    "\n",
    "    [Validation Rules]\n",
    "    - Respond \"VALID\" if all criteria are met\n",
    "    - Respond \"SEARCH FOR CONTENT: [Topic Area] - [Author] - [Specific Need]\" if:\n",
    "        * Missing key question aspects\n",
    "        * Contains generic/non-specific information\n",
    "        * Lacks author-style analysis\n",
    "\n",
    "    [Examples]\n",
    "    Good Response: VALID\n",
    "    Needs Improvement: SEARCH FOR CONTENT: real estate investing - Robert Kiyosaki - 2024 market trends\"\"\"\n",
    "\n",
    "    response = llm.invoke(feedback_prompt).content.strip()\n",
    "    print(f\"Raw validation response: {response}\")\n",
    "\n",
    "    # Process response\n",
    "    if \"VALID\" in response.upper():\n",
    "        state[\"validate_response\"] = \"VALID\"\n",
    "    else:\n",
    "        # Extract and clean search terms\n",
    "        search_terms = re.search(r\"SEARCH FOR CONTENT: (.+)\", response, re.IGNORECASE)\n",
    "        clean_terms = re.sub(r'[^a-zA-Z0-9\\s\\-,:]', '', search_terms.group(1))\n",
    "        state[\"validate_response\"] = f\"SEARCH FOR CONTENT: {clean_terms}\"\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_facts(state: State):\n",
    "\n",
    "        print(\"\\n=== Validating Factual Accuracy ===\")\n",
    "\n",
    "        validation_prompt = f\"\"\"**Fact Check Validation Task**\n",
    "\n",
    "        Author: {state['author']}\n",
    "        Topic: {state['topic']}\n",
    "        Question: {state['question']}\n",
    "\n",
    "        [Content to Validate]\n",
    "        {state['response_summary']} \n",
    "\n",
    "        [Validation Criteria]\n",
    "        1. Contains credible sources (citations, references)\n",
    "        2. Includes verifiable data/statistics\n",
    "        3. Aligns with {state['author']}'s known positions\n",
    "        4. Provides specific examples/evidence\n",
    "\n",
    "        [Response Format]\n",
    "        - 'True' if meets all criteria\n",
    "        - 'False' if any criteria not met\n",
    "\n",
    "        Answer:\"\"\"\n",
    "\n",
    "        response = llm.invoke(validation_prompt).content.strip().lower()\n",
    "        is_valid = \"true\" in response  # Handle variations like \"mostly true\"\n",
    "\n",
    "        print(f\"Fact check validation result: {is_valid}\")\n",
    "        state[\"validate_response\"] = is_valid\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fact_checks(state: State):\n",
    "    search = DuckDuckGoSearchRun()\n",
    "    search_query = \"\"\n",
    "\n",
    "    # Determine search terms\n",
    "    if \"SEARCH:\" in state[\"validate_response\"]:\n",
    "        # Extract specific search terms\n",
    "        search_terms = state[\"validate_response\"].split(\"SEARCH:\")[-1].strip()\n",
    "        search_query = f\"{state['author']} {search_terms}\"\n",
    "        print(f\"🔍 Using validation feedback terms: {search_terms}\")\n",
    "\n",
    "    # Execute search\n",
    "    results = search.invoke(search_query)\n",
    "    \n",
    "    # Store results with fallback\n",
    "    state[\"fact_correction\"] = results[:1000] if results else \"No additional information found\"\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_summary(state: State):\n",
    "   \n",
    "    if not state.get(\"objective_check\", True):\n",
    "            state[\"generate_final_response\"] = \"❌ Question not relevant to the topic\"\n",
    "            return state\n",
    "\n",
    "    # Handle failed fact checks\n",
    "    if not state.get(\"validate_response\", False):\n",
    "        state[\"generate_final_response\"] = \"⚠️ Unable to verify information credibility\"\n",
    "        return state\n",
    "\n",
    "    # Only proceed if both checks pass\n",
    "    prompt = f\"\"\"Synthesize a comprehensive answer in {state['author']}'s style \n",
    "    using these verified sources about {state['topic']}:\n",
    "\n",
    "    [Question] {state['question']}\n",
    "    [YouTube Analysis] {state['response_summary']}\n",
    "    [External Verification] {state['fact_correction']}\n",
    "\n",
    "    Requirements:\n",
    "    1. Begin with: \"As {state['author']}'s analytical clone specializing in {state['topic']}, my perspective is:\"\n",
    "    2. Maintain {state['author']}'s signature tone and terminology\n",
    "    3. Focus on practical applications for {state['topic']}\n",
    "    4. Highlight both opportunities and risks\n",
    "    5. Keep under 500 words\n",
    "\n",
    "    Final Analysis:\"\"\"\n",
    "        \n",
    "    state[\"generate_summary\"] = llm.invoke(prompt).content\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Building Combined Knowledge Base ===\n",
      "✅ Created combined index with 411 documents\n",
      "\n",
      "=== Generating Author-Style Response ===\n",
      "\n",
      "Final Analysis:\n",
      "In the current economic condition, the best investment is not a one-size-fits-all answer. As a student of financial literacy inspired by Robert Kiyosaki's teachings in Rich Dad Poor Dad, it is essential to understand that the economic landscape is ever-changing, presenting both challenges and opportunities.\n",
      "\n",
      "Looking back at historical examples, we can see that during times of economic turmoil, there were individuals who capitalized on the situation by being bold and taking calculated risks. For instance, in the early 1990s when the economy of Phoenix, Arizona was in a downturn, Robert Kiyosaki himself saw opportunities in real estate and the stock market when others were hesitant. This mindset of seizing opportunities when others are fearful is a key lesson to remember in any economic climate.\n",
      "\n",
      "In the current context, with market fluctuations and uncertainties, it is crucial to have a diversified investment portfolio that includes assets such as real estate, stocks, and possibly alternative investments like cryptocurrencies or precious metals. By spreading your investments across different asset classes, you can mitigate risks and potentially maximize returns.\n",
      "\n",
      "However, it is important to note that not all investments will yield high returns, and there are inherent risks involved in any investment strategy. It is essential to do thorough research, seek advice from financial experts, and continuously educate yourself on financial matters to make informed decisions.\n",
      "\n",
      "As Robert Kiyosaki often emphasizes, financial intelligence is key to identifying and seizing opportunities in any economic condition. By staying informed, being willing to go against the crowd, and being open to new possibilities, you can position yourself for success in the ever-evolving financial landscape.\n",
      "\n",
      "Remember, there is no one-size-fits-all answer to the question of the best investment in the current economic condition. It ultimately depends on your financial goals, risk tolerance, and level of financial intelligence. Stay proactive, stay informed, and be ready to adapt to changing circumstances to make the most of your investments.\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "state = get_topic_details()\n",
    "\n",
    "# Build knowledge base\n",
    "state = build_combined_index(state)\n",
    "\n",
    "# Get question\n",
    "state = get_user_question(state)\n",
    "\n",
    "# Generate response\n",
    "state = generate_author_response(state)\n",
    "\n",
    "print(f\"\\nFinal Analysis:\\n{state['response_summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_input(state: State):\n",
    "    \"\"\"\n",
    "    Route the input to the appropriate tool based on the input status\"\"\"\n",
    "\n",
    "    if state[\"objective_check\"] == True:\n",
    "        print(\"Accepted\")\n",
    "        return \"Accepted\"\n",
    "    elif state[\"objective_check\"] == False:\n",
    "        print(\"Rejected\")\n",
    "        return \"Rejected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_basedon_Summary(state: dict):\n",
    "    \"\"\"\n",
    "    Route the input to the appropriate tool based on the input status\n",
    "    \"\"\"\n",
    "    validate_yt_summary = state.get(\"validate_yt_summary\", \"\")\n",
    "\n",
    "    if validate_yt_summary == \"Valid\":\n",
    "        print(\"Accepted\")\n",
    "        return \"Accepted\"\n",
    "    else: \"SEARCH\" in validate_yt_summary\n",
    "    print(\"need more info\")\n",
    "    return \"need more info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_factcheck(state: State):\n",
    "    \"\"\"\n",
    "    Route the input to the appropriate tool based on the factcheck status\"\"\"\n",
    "\n",
    "    if state[\"validate_fact_checks\"] == True:\n",
    "        print(\"Accepted\")\n",
    "        return \"Accepted\"\n",
    "    elif state[\"validate_fact_checks\"] == False:\n",
    "        print(\"Rejected\")\n",
    "        return \"Rejected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import START, StateGraph,END\n",
    "from langgraph.prebuilt import tools_condition, ToolNode\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Graphbuilder = StateGraph(State)\n",
    "Graphbuilder.add_node(\"get_user_input\", get_user_input)\n",
    "Graphbuilder.add_node(\"YouTube_search\",YouTube_search)\n",
    "Graphbuilder.add_node(\"validate_user_input\", validate_user_input)\n",
    "Graphbuilder.add_node(\"validate_ytsummary\", validate_ytsummary)\n",
    "Graphbuilder.add_node(\"fact_checks\", fact_checks)\n",
    "Graphbuilder.add_node(\"consolidate_summary\", consolidate_summary)\n",
    "Graphbuilder.add_node(\"validate_facts\", validate_facts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
